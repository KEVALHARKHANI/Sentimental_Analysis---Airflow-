{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1494770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import pickle\n",
    "\n",
    "import re, string, random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95451cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "def Mongoconnect():\n",
    "    URL = f\"mongodb://host.docker.internal/\"\n",
    "    url = f\"mongodb://localhost:27017/\"\n",
    "    client = MongoClient(url)\n",
    "    db = client.admin\n",
    "    # Issue the serverStatus command and print the results\n",
    "    serverStatusResult = db.command(\"serverStatus\")\n",
    "    print(serverStatusResult)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9292a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'host': 'kevals-MacBook-Air.local', 'version': '5.0.3', 'process': 'mongod', 'pid': 9508, 'uptime': 16255.0, 'uptimeMillis': 16256062, 'uptimeEstimate': 16256, 'localTime': datetime.datetime(2022, 6, 5, 20, 12, 27, 945000), 'asserts': {'regular': 0, 'warning': 0, 'msg': 0, 'user': 894, 'tripwire': 0, 'rollovers': 0}, 'catalogStats': {'collections': 18, 'capped': 0, 'timeseries': 0, 'views': 0, 'internalCollections': 3, 'internalViews': 0}, 'connections': {'current': 41, 'available': 51159, 'totalCreated': 307, 'active': 8, 'threaded': 41, 'exhaustIsMaster': 0, 'exhaustHello': 6, 'awaitingTopologyChanges': 7}, 'electionMetrics': {'stepUpCmd': {'called': 0, 'successful': 0}, 'priorityTakeover': {'called': 0, 'successful': 0}, 'catchUpTakeover': {'called': 0, 'successful': 0}, 'electionTimeout': {'called': 0, 'successful': 0}, 'freezeTimeout': {'called': 0, 'successful': 0}, 'numStepDownsCausedByHigherTerm': 0, 'numCatchUps': 0, 'numCatchUpsSucceeded': 0, 'numCatchUpsAlreadyCaughtUp': 0, 'numCatchUpsSkipped': 0, 'numCatchUpsTimedOut': 0, 'numCatchUpsFailedWithError': 0, 'numCatchUpsFailedWithNewTerm': 0, 'numCatchUpsFailedWithReplSetAbortPrimaryCatchUpCmd': 0, 'averageCatchUpOps': 0.0}, 'extra_info': {'note': 'fields vary by platform', 'page_faults': 31058}, 'flowControl': {'enabled': True, 'targetRateLimit': 1000000000, 'timeAcquiringMicros': 4111, 'locksPerKiloOp': 0.0, 'sustainerRate': 0, 'isLagged': False, 'isLaggedCount': 0, 'isLaggedTimeMicros': 0}, 'freeMonitoring': {'state': 'undecided'}, 'globalLock': {'totalTime': 16255990000, 'currentQueue': {'total': 0, 'readers': 0, 'writers': 0}, 'activeClients': {'total': 0, 'readers': 0, 'writers': 0}}, 'locks': {'ParallelBatchWriterMode': {'acquireCount': {'r': 700}}, 'ReplicationStateTransition': {'acquireCount': {'w': 62017}}, 'Global': {'acquireCount': {'r': 61840, 'w': 427, 'W': 5}}, 'Database': {'acquireCount': {'r': 55, 'w': 426, 'W': 1}}, 'Collection': {'acquireCount': {'r': 123, 'w': 424, 'W': 2}}, 'Mutex': {'acquireCount': {'r': 719}}}, 'logicalSessionRecordCache': {'activeSessionsCount': 1, 'sessionsCollectionJobCount': 53, 'lastSessionsCollectionJobDurationMillis': 2, 'lastSessionsCollectionJobTimestamp': datetime.datetime(2022, 6, 5, 20, 11, 0, 148000), 'lastSessionsCollectionJobEntriesRefreshed': 0, 'lastSessionsCollectionJobEntriesEnded': 0, 'lastSessionsCollectionJobCursorsClosed': 0, 'transactionReaperJobCount': 53, 'lastTransactionReaperJobDurationMillis': 2, 'lastTransactionReaperJobTimestamp': datetime.datetime(2022, 6, 5, 20, 11, 0, 214000), 'lastTransactionReaperJobEntriesCleanedUp': 0, 'sessionCatalogSize': 0}, 'network': {'bytesIn': 1355546, 'bytesOut': 8397697, 'physicalBytesIn': 486348, 'physicalBytesOut': 8397109, 'numSlowDNSOperations': 0, 'numSlowSSLOperations': 0, 'numRequests': 13596, 'tcpFastOpen': {'serverSupported': False, 'clientSupported': False, 'accepted': 0}, 'compression': {'snappy': {'compressor': {'bytesIn': 0, 'bytesOut': 0}, 'decompressor': {'bytesIn': 0, 'bytesOut': 0}}, 'zstd': {'compressor': {'bytesIn': 0, 'bytesOut': 0}, 'decompressor': {'bytesIn': 0, 'bytesOut': 0}}, 'zlib': {'compressor': {'bytesIn': 0, 'bytesOut': 0}, 'decompressor': {'bytesIn': 0, 'bytesOut': 0}}}, 'serviceExecutors': {'passthrough': {'threadsRunning': 41, 'clientsInTotal': 41, 'clientsRunning': 41, 'clientsWaitingForData': 0}, 'fixed': {'threadsRunning': 1, 'clientsInTotal': 0, 'clientsRunning': 0, 'clientsWaitingForData': 0}}}, 'opLatencies': {'reads': {'latency': 3574059, 'ops': 32}, 'writes': {'latency': 14861, 'ops': 10}, 'commands': {'latency': 83267651, 'ops': 13546}, 'transactions': {'latency': 0, 'ops': 0}}, 'opcounters': {'insert': 10, 'query': 76, 'update': 131, 'delete': 9, 'getmore': 0, 'command': 13658}, 'opcountersRepl': {'insert': 0, 'query': 0, 'update': 0, 'delete': 0, 'getmore': 0, 'command': 0}, 'readConcernCounters': {'nonTransactionOps': {'none': 32, 'noneInfo': {'CWRC': {'local': 0, 'available': 0, 'majority': 0}, 'implicitDefault': {'local': 32, 'available': 0}}, 'local': 0, 'available': 0, 'majority': 0, 'snapshot': {'withClusterTime': 0, 'withoutClusterTime': 0}, 'linearizable': 0}, 'transactionOps': {'none': 0, 'noneInfo': {'CWRC': {'local': 0, 'majority': 0}, 'implicitDefault': {'local': 0}}, 'local': 0, 'majority': 0, 'snapshot': {'withClusterTime': 0, 'withoutClusterTime': 0}}}, 'security': {'authentication': {'saslSupportedMechsReceived': 1, 'mechanisms': {'MONGODB-X509': {'speculativeAuthenticate': {'received': 0, 'successful': 0}, 'clusterAuthenticate': {'received': 0, 'successful': 0}, 'authenticate': {'received': 0, 'successful': 0}}, 'SCRAM-SHA-1': {'speculativeAuthenticate': {'received': 0, 'successful': 0}, 'clusterAuthenticate': {'received': 0, 'successful': 0}, 'authenticate': {'received': 1, 'successful': 0}}, 'SCRAM-SHA-256': {'speculativeAuthenticate': {'received': 1, 'successful': 0}, 'clusterAuthenticate': {'received': 0, 'successful': 0}, 'authenticate': {'received': 1, 'successful': 0}}}}}, 'storageEngine': {'name': 'wiredTiger', 'supportsCommittedReads': True, 'oldestRequiredTimestampForCrashRecovery': Timestamp(0, 0), 'supportsPendingDrops': True, 'dropPendingIdents': 0, 'supportsSnapshotReadConcern': True, 'readOnly': False, 'persistent': True, 'backupCursorOpen': False, 'supportsResumableIndexBuilds': True}, 'tenantMigrations': {'currentMigrationsDonating': 0, 'currentMigrationsReceiving': 0, 'totalSuccessfulMigrationsDonated': 0, 'totalSuccessfulMigrationsReceived': 0, 'totalFailedMigrationsDonated': 0, 'totalFailedMigrationsReceived': 0}, 'trafficRecording': {'running': False}, 'transactions': {'retriedCommandsCount': 0, 'retriedStatementsCount': 0, 'transactionsCollectionWriteCount': 0, 'currentActive': 0, 'currentInactive': 0, 'currentOpen': 0, 'totalAborted': 0, 'totalCommitted': 0, 'totalStarted': 0, 'totalPrepared': 0, 'totalPreparedThenCommitted': 0, 'totalPreparedThenAborted': 0, 'currentPrepared': 0}, 'transportSecurity': {'1.0': 0, '1.1': 0, '1.2': 0, '1.3': 0, 'unknown': 0}, 'twoPhaseCommitCoordinator': {'totalCreated': 0, 'totalStartedTwoPhaseCommit': 0, 'totalAbortedTwoPhaseCommit': 0, 'totalCommittedTwoPhaseCommit': 0, 'currentInSteps': {'writingParticipantList': 0, 'waitingForVotes': 0, 'writingDecision': 0, 'waitingForDecisionAcks': 0, 'deletingCoordinatorDoc': 0}}, 'wiredTiger': {'uri': 'statistics:', 'block-manager': {'blocks pre-loaded': 48, 'blocks read': 480, 'blocks written': 1931, 'bytes read': 2048000, 'bytes read via memory map API': 0, 'bytes read via system call API': 0, 'bytes written': 19222528, 'bytes written for checkpoint': 19222528, 'bytes written via memory map API': 0, 'bytes written via system call API': 0, 'mapped blocks read': 0, 'mapped bytes read': 0, 'number of times the file was remapped because it changed size via fallocate or truncate': 0, 'number of times the region was remapped via write': 0}, 'cache': {'application threads page read from disk to cache count': 6, 'application threads page read from disk to cache time (usecs)': 289, 'application threads page write from cache to disk count': 1102, 'application threads page write from cache to disk time (usecs)': 7780840, 'bytes allocated for updates': 126971, 'bytes belonging to page images in the cache': 155205, 'bytes belonging to the history store table in the cache': 597, 'bytes currently in the cache': 297681, 'bytes dirty in the cache cumulative': 20958090, 'bytes not belonging to page images in the cache': 142475, 'bytes read into cache': 143709, 'bytes written from cache': 13080386, 'cache overflow score': 0, 'checkpoint blocked page eviction': 0, 'checkpoint of history store file blocked non-history store page eviction': 0, 'eviction calls to get a page': 2019, 'eviction calls to get a page found queue empty': 1992, 'eviction calls to get a page found queue empty after locking': 4, 'eviction currently operating in aggressive mode': 0, 'eviction empty score': 0, 'eviction gave up due to detecting an out of order on disk value behind the last update on the chain': 0, 'eviction gave up due to detecting an out of order tombstone ahead of the selected on disk update': 0, 'eviction gave up due to detecting an out of order tombstone ahead of the selected on disk update after validating the update chain': 0, 'eviction gave up due to detecting out of order timestamps on the update chain after the selected on disk update': 0, 'eviction passes of a file': 0, 'eviction server candidate queue empty when topping up': 0, 'eviction server candidate queue not empty when topping up': 0, 'eviction server evicting pages': 0, 'eviction server slept, because we did not make progress with eviction': 14, 'eviction server unable to reach eviction goal': 0, 'eviction server waiting for a leaf page': 2, 'eviction state': 64, 'eviction walk target pages histogram - 0-9': 0, 'eviction walk target pages histogram - 10-31': 0, 'eviction walk target pages histogram - 128 and higher': 0, 'eviction walk target pages histogram - 32-63': 0, 'eviction walk target pages histogram - 64-128': 0, 'eviction walk target pages reduced due to history store cache pressure': 0, 'eviction walk target strategy both clean and dirty pages': 0, 'eviction walk target strategy only clean pages': 0, 'eviction walk target strategy only dirty pages': 0, 'eviction walks abandoned': 0, 'eviction walks gave up because they restarted their walk twice': 0, 'eviction walks gave up because they saw too many pages and found no candidates': 0, 'eviction walks gave up because they saw too many pages and found too few candidates': 0, 'eviction walks reached end of tree': 0, 'eviction walks restarted': 0, 'eviction walks started from root of tree': 0, 'eviction walks started from saved location in tree': 0, 'eviction worker thread active': 4, 'eviction worker thread created': 0, 'eviction worker thread evicting pages': 20, 'eviction worker thread removed': 0, 'eviction worker thread stable number': 0, 'files with active eviction walks': 0, 'files with new eviction walks started': 0, 'force re-tuning of eviction workers once in a while': 0, 'forced eviction - history store pages failed to evict while session has history store cursor open': 0, 'forced eviction - history store pages selected while session has history store cursor open': 0, 'forced eviction - history store pages successfully evicted while session has history store cursor open': 0, 'forced eviction - pages evicted that were clean count': 0, 'forced eviction - pages evicted that were clean time (usecs)': 0, 'forced eviction - pages evicted that were dirty count': 0, 'forced eviction - pages evicted that were dirty time (usecs)': 0, 'forced eviction - pages selected because of a large number of updates to a single item': 0, 'forced eviction - pages selected because of too many deleted items count': 0, 'forced eviction - pages selected count': 0, 'forced eviction - pages selected unable to be evicted count': 0, 'forced eviction - pages selected unable to be evicted time': 0, 'hazard pointer blocked page eviction': 0, 'hazard pointer check calls': 20, 'hazard pointer check entries walked': 7, 'hazard pointer maximum array length': 1, 'history store score': 0, 'history store table insert calls': 0, 'history store table insert calls that returned restart': 0, 'history store table max on-disk size': 0, 'history store table on-disk size': 4096, 'history store table out-of-order resolved updates that lose their durable timestamp': 0, 'history store table out-of-order updates that were fixed up by reinserting with the fixed timestamp': 0, 'history store table reads': 0, 'history store table reads missed': 0, 'history store table reads requiring squashed modifies': 0, 'history store table truncation by rollback to stable to remove an unstable update': 0, 'history store table truncation by rollback to stable to remove an update': 0, 'history store table truncation to remove an update': 0, 'history store table truncation to remove range of updates due to key being removed from the data page during reconciliation': 0, 'history store table truncation to remove range of updates due to out-of-order timestamp update on data page': 0, 'history store table writes requiring squashed modifies': 0, 'in-memory page passed criteria to be split': 0, 'in-memory page splits': 0, 'internal pages evicted': 0, 'internal pages queued for eviction': 0, 'internal pages seen by eviction walk': 0, 'internal pages seen by eviction walk that are already queued': 0, 'internal pages split during eviction': 0, 'leaf pages split during eviction': 0, 'maximum bytes configured': 3758096384, 'maximum page size at eviction': 0, 'modified pages evicted': 20, 'modified pages evicted by application threads': 0, 'operations timed out waiting for space in cache': 0, 'overflow pages read into cache': 0, 'page split during eviction deepened the tree': 0, 'page written requiring history store records': 0, 'pages currently held in the cache': 63, 'pages evicted by application threads': 0, 'pages evicted in parallel with checkpoint': 20, 'pages queued for eviction': 0, 'pages queued for eviction post lru sorting': 0, 'pages queued for urgent eviction': 20, 'pages queued for urgent eviction during walk': 0, 'pages queued for urgent eviction from history store due to high dirty content': 0, 'pages read into cache': 45, 'pages read into cache after truncate': 26, 'pages read into cache after truncate in prepare state': 0, 'pages requested from the cache': 17632, 'pages seen by eviction walk': 0, 'pages seen by eviction walk that are already queued': 0, 'pages selected for eviction unable to be evicted': 0, 'pages selected for eviction unable to be evicted as the parent page has overflow items': 0, 'pages selected for eviction unable to be evicted because of active children on an internal page': 0, 'pages selected for eviction unable to be evicted because of failure in reconciliation': 0, 'pages selected for eviction unable to be evicted because of race between checkpoint and out of order timestamps handling': 0, 'pages walked for eviction': 0, 'pages written from cache': 1105, 'pages written requiring in-memory restoration': 3, 'percentage overhead': 8, 'tracked bytes belonging to internal pages in the cache': 14450, 'tracked bytes belonging to leaf pages in the cache': 283231, 'tracked dirty bytes in the cache': 414, 'tracked dirty pages in the cache': 1, 'unmodified pages evicted': 0}, 'capacity': {'background fsync file handles considered': 0, 'background fsync file handles synced': 0, 'background fsync time (msecs)': 0, 'bytes read': 266240, 'bytes written for checkpoint': 12968877, 'bytes written for eviction': 0, 'bytes written for log': 1243404032, 'bytes written total': 1256372909, 'threshold to call fsync': 0, 'time waiting due to total capacity (usecs)': 0, 'time waiting during checkpoint (usecs)': 0, 'time waiting during eviction (usecs)': 0, 'time waiting during logging (usecs)': 0, 'time waiting during read (usecs)': 0}, 'checkpoint-cleanup': {'pages added for eviction': 20, 'pages removed': 0, 'pages skipped during tree walk': 0, 'pages visited': 1197}, 'connection': {'auto adjusting condition resets': 730, 'auto adjusting condition wait calls': 89615, 'auto adjusting condition wait raced to update timeout and skipped updating': 0, 'detected system time went backwards': 0, 'files currently open': 50, 'hash bucket array size for data handles': 512, 'hash bucket array size general': 512, 'memory allocations': 610540, 'memory frees': 606651, 'memory re-allocations': 66784, 'pthread mutex condition wait calls': 187496, 'pthread mutex shared lock read-lock calls': 275155, 'pthread mutex shared lock write-lock calls': 17431, 'total fsync I/Os': 1749, 'total read I/Os': 2022, 'total write I/Os': 2733}, 'cursor': {'Total number of entries skipped by cursor next calls': 10796, 'Total number of entries skipped by cursor prev calls': 188, 'Total number of entries skipped to position the history store cursor': 0, 'Total number of pages skipped without reading by cursor next calls': 0, 'Total number of pages skipped without reading by cursor prev calls': 0, 'Total number of times a search near has exited due to prefix config': 0, 'cached cursor count': 5, 'cursor bulk loaded cursor insert calls': 0, 'cursor close calls that result in cache': 120774, 'cursor create calls': 211, 'cursor insert calls': 1404, 'cursor insert key and value bytes': 636782, 'cursor modify calls': 0, 'cursor modify key and value bytes affected': 0, 'cursor modify value bytes modified': 0, 'cursor next calls': 1258, 'cursor next calls that skip due to a globally visible history store tombstone': 0, 'cursor next calls that skip greater than or equal to 100 entries': 28, 'cursor next calls that skip less than 100 entries': 1228, 'cursor operation restarted': 0, 'cursor prev calls': 74, 'cursor prev calls that skip due to a globally visible history store tombstone': 0, 'cursor prev calls that skip greater than or equal to 100 entries': 0, 'cursor prev calls that skip less than 100 entries': 74, 'cursor remove calls': 342, 'cursor remove key bytes removed': 9057, 'cursor reserve calls': 0, 'cursor reset calls': 137159, 'cursor search calls': 2633, 'cursor search history store calls': 0, 'cursor search near calls': 606, 'cursor sweep buckets': 20502, 'cursor sweep cursors closed': 0, 'cursor sweep cursors examined': 16, 'cursor sweeps': 3417, 'cursor truncate calls': 0, 'cursor update calls': 0, 'cursor update key and value bytes': 0, 'cursor update value size change': 0, 'cursors reused from cache': 120769, 'open cursor count': 8}, 'data-handle': {'connection data handle size': 464, 'connection data handles currently active': 93, 'connection sweep candidate became referenced': 0, 'connection sweep dhandles closed': 0, 'connection sweep dhandles removed from hash list': 424, 'connection sweep time-of-death sets': 2477, 'connection sweeps': 1556, 'connection sweeps skipped due to checkpoint gathering handles': 1, 'session dhandles swept': 5878, 'session sweep attempts': 392}, 'lock': {'checkpoint lock acquisitions': 260, 'checkpoint lock application thread wait time (usecs)': 21554, 'checkpoint lock internal thread wait time (usecs)': 0, 'dhandle lock application thread time waiting (usecs)': 17, 'dhandle lock internal thread time waiting (usecs)': 12, 'dhandle read lock acquisitions': 62164, 'dhandle write lock acquisitions': 945, 'durable timestamp queue lock application thread time waiting (usecs)': 0, 'durable timestamp queue lock internal thread time waiting (usecs)': 0, 'durable timestamp queue read lock acquisitions': 0, 'durable timestamp queue write lock acquisitions': 0, 'metadata lock acquisitions': 259, 'metadata lock application thread wait time (usecs)': 53, 'metadata lock internal thread wait time (usecs)': 0, 'read timestamp queue lock application thread time waiting (usecs)': 0, 'read timestamp queue lock internal thread time waiting (usecs)': 0, 'read timestamp queue read lock acquisitions': 0, 'read timestamp queue write lock acquisitions': 0, 'schema lock acquisitions': 353, 'schema lock application thread wait time (usecs)': 172519, 'schema lock internal thread wait time (usecs)': 0, 'table lock application thread time waiting for the table lock (usecs)': 78, 'table lock internal thread time waiting for the table lock (usecs)': 0, 'table read lock acquisitions': 0, 'table write lock acquisitions': 264, 'txn global lock application thread time waiting (usecs)': 2, 'txn global lock internal thread time waiting (usecs)': 0, 'txn global read lock acquisitions': 961, 'txn global write lock acquisitions': 702}, 'log': {'busy returns attempting to switch slots': 45104, 'force archive time sleeping (usecs)': 0, 'log bytes of payload data': 360082, 'log bytes written': 451584, 'log files manually zero-filled': 0, 'log flush operations': 92990, 'log force write operations': 108819, 'log force write operations skipped': 108541, 'log records compressed': 265, 'log records not compressed': 142, 'log records too small to compress': 671, 'log release advances write LSN': 262, 'log scan operations': 6, 'log scan records requiring two reads': 0, 'log server thread advances write LSN': 275, 'log server thread write LSN walk skipped': 16307, 'log sync operations': 535, 'log sync time duration (usecs)': 37739388, 'log sync_dir operations': 1, 'log sync_dir time duration (usecs)': 17894, 'log write operations': 1078, 'logging bytes consolidated': 451072, 'maximum log file size': 104857600, 'number of pre-allocated log files to create': 2, 'pre-allocated log files not ready and missed': 1, 'pre-allocated log files prepared': 2, 'pre-allocated log files used': 0, 'records processed by log scan': 13, 'slot close lost race': 0, 'slot close unbuffered waits': 0, 'slot closures': 537, 'slot join atomic update races': 0, 'slot join calls atomic updates raced': 0, 'slot join calls did not yield': 1078, 'slot join calls found active slot closed': 0, 'slot join calls slept': 0, 'slot join calls yielded': 0, 'slot join found active slot closed': 0, 'slot joins yield time (usecs)': 0, 'slot transitions unable to find free slot': 0, 'slot unbuffered writes': 0, 'total in-memory size of compressed records': 616126, 'total log buffer size': 33554432, 'total size of compressed records': 302493, 'written slots coalesced': 0, 'yields waiting for previous log file close': 0}, 'perf': {'file system read latency histogram (bucket 1) - 10-49ms': 52, 'file system read latency histogram (bucket 2) - 50-99ms': 0, 'file system read latency histogram (bucket 3) - 100-249ms': 0, 'file system read latency histogram (bucket 4) - 250-499ms': 0, 'file system read latency histogram (bucket 5) - 500-999ms': 0, 'file system read latency histogram (bucket 6) - 1000ms+': 0, 'file system write latency histogram (bucket 1) - 10-49ms': 187, 'file system write latency histogram (bucket 2) - 50-99ms': 19, 'file system write latency histogram (bucket 3) - 100-249ms': 1, 'file system write latency histogram (bucket 4) - 250-499ms': 1, 'file system write latency histogram (bucket 5) - 500-999ms': 0, 'file system write latency histogram (bucket 6) - 1000ms+': 0, 'operation read latency histogram (bucket 1) - 100-249us': 72, 'operation read latency histogram (bucket 2) - 250-499us': 12, 'operation read latency histogram (bucket 3) - 500-999us': 7, 'operation read latency histogram (bucket 4) - 1000-9999us': 15, 'operation read latency histogram (bucket 5) - 10000us+': 36, 'operation write latency histogram (bucket 1) - 100-249us': 14, 'operation write latency histogram (bucket 2) - 250-499us': 12, 'operation write latency histogram (bucket 3) - 500-999us': 11, 'operation write latency histogram (bucket 4) - 1000-9999us': 10, 'operation write latency histogram (bucket 5) - 10000us+': 26}, 'reconciliation': {'approximate byte size of timestamps in pages written': 0, 'approximate byte size of transaction IDs in pages written': 7056, 'fast-path pages deleted': 0, 'internal-page overflow keys': 0, 'leaf-page overflow keys': 0, 'maximum seconds spent in a reconciliation call': 0, 'page reconciliation calls': 1167, 'page reconciliation calls for eviction': 20, 'page reconciliation calls that resulted in values with prepared transaction metadata': 0, 'page reconciliation calls that resulted in values with timestamps': 0, 'page reconciliation calls that resulted in values with transaction ids': 525, 'pages deleted': 62, 'pages written including an aggregated newest start durable timestamp ': 0, 'pages written including an aggregated newest stop durable timestamp ': 0, 'pages written including an aggregated newest stop timestamp ': 0, 'pages written including an aggregated newest stop transaction ID': 0, 'pages written including an aggregated newest transaction ID ': 0, 'pages written including an aggregated oldest start timestamp ': 0, 'pages written including an aggregated prepare': 0, 'pages written including at least one prepare state': 0, 'pages written including at least one start durable timestamp': 0, 'pages written including at least one start timestamp': 0, 'pages written including at least one start transaction ID': 525, 'pages written including at least one stop durable timestamp': 0, 'pages written including at least one stop timestamp': 0, 'pages written including at least one stop transaction ID': 0, 'records written including a prepare state': 0, 'records written including a start durable timestamp': 0, 'records written including a start timestamp': 0, 'records written including a start transaction ID': 882, 'records written including a stop durable timestamp': 0, 'records written including a stop timestamp': 0, 'records written including a stop transaction ID': 0, 'split bytes currently awaiting free': 0, 'split objects currently awaiting free': 0}, 'session': {'flush state races': 0, 'flush_tier operation calls': 0, 'open session count': 16, 'session query timestamp calls': 0, 'table alter failed calls': 0, 'table alter successful calls': 0, 'table alter unchanged and skipped': 0, 'table compact failed calls': 0, 'table compact successful calls': 0, 'table create failed calls': 0, 'table create successful calls': 3, 'table drop failed calls': 0, 'table drop successful calls': 0, 'table rename failed calls': 0, 'table rename successful calls': 0, 'table salvage failed calls': 0, 'table salvage successful calls': 0, 'table truncate failed calls': 0, 'table truncate successful calls': 0, 'table verify failed calls': 0, 'table verify successful calls': 0, 'tiered operations dequeued and processed': 0, 'tiered operations scheduled': 0, 'tiered storage local retention time (secs)': 0, 'tiered storage object size': 0}, 'thread-state': {'active filesystem fsync calls': 0, 'active filesystem read calls': 0, 'active filesystem write calls': 0}, 'thread-yield': {'application thread time evicting (usecs)': 0, 'application thread time waiting for cache (usecs)': 0, 'connection close blocked waiting for transaction state stabilization': 0, 'connection close yielded for lsm manager shutdown': 0, 'data handle lock yielded': 0, 'get reference for page index and slot time sleeping (usecs)': 0, 'page access yielded due to prepare state change': 0, 'page acquire busy blocked': 0, 'page acquire eviction blocked': 0, 'page acquire locked blocked': 0, 'page acquire read blocked': 0, 'page acquire time sleeping (usecs)': 0, 'page delete rollback time sleeping for state change (usecs)': 0, 'page reconciliation yielded due to child modification': 0}, 'transaction': {'Number of prepared updates': 0, 'Number of prepared updates committed': 0, 'Number of prepared updates repeated on the same key': 0, 'Number of prepared updates rolled back': 0, 'prepared transactions': 0, 'prepared transactions committed': 0, 'prepared transactions currently active': 0, 'prepared transactions rolled back': 0, 'prepared transactions rolled back and do not remove the history store entry': 0, 'prepared transactions rolled back and fix the history store entry with checkpoint reserved transaction id': 0, 'query timestamp calls': 14676, 'race to read prepared update retry': 0, 'rollback to stable calls': 0, 'rollback to stable history store records with stop timestamps older than newer records': 0, 'rollback to stable inconsistent checkpoint': 0, 'rollback to stable keys removed': 0, 'rollback to stable keys restored': 0, 'rollback to stable pages visited': 1, 'rollback to stable restored tombstones from history store': 0, 'rollback to stable restored updates from history store': 0, 'rollback to stable skipping delete rle': 0, 'rollback to stable skipping stable rle': 0, 'rollback to stable sweeping history store keys': 0, 'rollback to stable tree walk skipping pages': 0, 'rollback to stable updates aborted': 0, 'rollback to stable updates removed from history store': 0, 'sessions scanned in each walk of concurrent sessions': 338940, 'set timestamp calls': 0, 'set timestamp durable calls': 0, 'set timestamp durable updates': 0, 'set timestamp oldest calls': 0, 'set timestamp oldest updates': 0, 'set timestamp stable calls': 0, 'set timestamp stable updates': 0, 'transaction begins': 869, 'transaction checkpoint currently running': 0, 'transaction checkpoint currently running for history store file': 0, 'transaction checkpoint generation': 260, 'transaction checkpoint history store file duration (usecs)': 211, 'transaction checkpoint max time (msecs)': 9218, 'transaction checkpoint min time (msecs)': 30, 'transaction checkpoint most recent duration for gathering all handles (usecs)': 1741, 'transaction checkpoint most recent duration for gathering applied handles (usecs)': 0, 'transaction checkpoint most recent duration for gathering skipped handles (usecs)': 1111, 'transaction checkpoint most recent handles applied': 1, 'transaction checkpoint most recent handles skipped': 45, 'transaction checkpoint most recent handles walked': 93, 'transaction checkpoint most recent time (msecs)': 78, 'transaction checkpoint prepare currently running': 0, 'transaction checkpoint prepare max time (msecs)': 3627, 'transaction checkpoint prepare min time (msecs)': 0, 'transaction checkpoint prepare most recent time (msecs)': 1, 'transaction checkpoint prepare total time (msecs)': 16854, 'transaction checkpoint scrub dirty target': 0, 'transaction checkpoint scrub time (msecs)': 0, 'transaction checkpoint total time (msecs)': 117796, 'transaction checkpoints': 259, 'transaction checkpoints due to obsolete pages': 0, 'transaction checkpoints skipped because database was clean': 0, 'transaction failures due to history store': 0, 'transaction fsync calls for checkpoint after allocating the transaction ID': 259, 'transaction fsync duration for checkpoint after allocating the transaction ID (usecs)': 18990, 'transaction range of IDs currently pinned': 0, 'transaction range of IDs currently pinned by a checkpoint': 0, 'transaction range of timestamps currently pinned': 0, 'transaction range of timestamps pinned by a checkpoint': 0, 'transaction range of timestamps pinned by the oldest active read timestamp': 0, 'transaction range of timestamps pinned by the oldest timestamp': 0, 'transaction read timestamp of the oldest active reader': 0, 'transaction rollback to stable currently running': 0, 'transaction walk of concurrent sessions': 17534, 'transactions committed': 293, 'transactions rolled back': 576, 'update conflicts': 0}, 'concurrentTransactions': {'write': {'out': 0, 'available': 128, 'totalTickets': 128}, 'read': {'out': 1, 'available': 127, 'totalTickets': 128}}, 'snapshot-window-settings': {'total number of SnapshotTooOld errors': 0, 'minimum target snapshot window size in seconds': 300, 'current available snapshot window size in seconds': 0, 'latest majority snapshot timestamp available': 'Dec 31 19:00:00:0', 'oldest majority snapshot timestamp available': 'Dec 31 19:00:00:0', 'pinned timestamp requests': 0, 'min pinned timestamp': Timestamp(4294967295, 4294967295)}, 'oplog': {'visibility timestamp': Timestamp(0, 0)}}, 'mem': {'bits': 64, 'resident': 16, 'virtual': 35834, 'supported': True}, 'metrics': {'apiVersions': {'': ['default'], 'MongoDB Compass': ['default']}, 'aggStageCounters': {'$_internalConvertBucketIndexStats': 0, '$_internalFindAndModifyImageLookup': 0, '$_internalInhibitOptimization': 0, '$_internalReshardingIterateTransaction': 0, '$_internalReshardingOwnershipMatch': 0, '$_internalSetWindowFields': 0, '$_internalSplitPipeline': 0, '$_internalUnpackBucket': 0, '$_unpackBucket': 0, '$addFields': 0, '$bucket': 0, '$bucketAuto': 0, '$changeStream': 0, '$collStats': 2, '$count': 2, '$currentOp': 3, '$documents': 0, '$facet': 0, '$geoNear': 0, '$graphLookup': 0, '$group': 5, '$indexStats': 2, '$limit': 2, '$listLocalSessions': 0, '$listSessions': 0, '$lookup': 0, '$match': 5, '$merge': 0, '$mergeCursors': 0, '$operationMetrics': 0, '$out': 0, '$planCacheStats': 0, '$project': 2, '$queue': 0, '$redact': 0, '$replaceRoot': 0, '$replaceWith': 0, '$sample': 1, '$set': 131, '$setWindowFields': 0, '$skip': 0, '$sort': 0, '$sortByCount': 0, '$unionWith': 0, '$unset': 0, '$unwind': 0}, 'commands': {'<UNKNOWN>': 11, '_addShard': {'failed': 0, 'total': 0}, '_cloneCollectionOptionsFromPrimaryShard': {'failed': 0, 'total': 0}, '_configsvrAbortReshardCollection': {'failed': 0, 'total': 0}, '_configsvrAddShard': {'failed': 0, 'total': 0}, '_configsvrAddShardToZone': {'failed': 0, 'total': 0}, '_configsvrBalancerCollectionStatus': {'failed': 0, 'total': 0}, '_configsvrBalancerStart': {'failed': 0, 'total': 0}, '_configsvrBalancerStatus': {'failed': 0, 'total': 0}, '_configsvrBalancerStop': {'failed': 0, 'total': 0}, '_configsvrCleanupReshardCollection': {'failed': 0, 'total': 0}, '_configsvrClearJumboFlag': {'failed': 0, 'total': 0}, '_configsvrCommitChunkMerge': {'failed': 0, 'total': 0}, '_configsvrCommitChunkMigration': {'failed': 0, 'total': 0}, '_configsvrCommitChunkSplit': {'failed': 0, 'total': 0}, '_configsvrCommitChunksMerge': {'failed': 0, 'total': 0}, '_configsvrCommitMovePrimary': {'failed': 0, 'total': 0}, '_configsvrCommitReshardCollection': {'failed': 0, 'total': 0}, '_configsvrCreateDatabase': {'failed': 0, 'total': 0}, '_configsvrDropCollection': {'failed': 0, 'total': 0}, '_configsvrDropDatabase': {'failed': 0, 'total': 0}, '_configsvrEnableSharding': {'failed': 0, 'total': 0}, '_configsvrEnsureChunkVersionIsGreaterThan': {'failed': 0, 'total': 0}, '_configsvrMoveChunk': {'failed': 0, 'total': 0}, '_configsvrMovePrimary': {'failed': 0, 'total': 0}, '_configsvrRefineCollectionShardKey': {'failed': 0, 'total': 0}, '_configsvrRemoveChunks': {'failed': 0, 'total': 0}, '_configsvrRemoveShard': {'failed': 0, 'total': 0}, '_configsvrRemoveShardFromZone': {'failed': 0, 'total': 0}, '_configsvrRemoveTags': {'failed': 0, 'total': 0}, '_configsvrRenameCollectionMetadata': {'failed': 0, 'total': 0}, '_configsvrReshardCollection': {'failed': 0, 'total': 0}, '_configsvrSetAllowMigrations': {'failed': 0, 'total': 0}, '_configsvrShardCollection': {'failed': 0, 'total': 0}, '_configsvrUpdateZoneKeyRange': {'failed': 0, 'total': 0}, '_flushDatabaseCacheUpdates': {'failed': 0, 'total': 0}, '_flushDatabaseCacheUpdatesWithWriteConcern': {'failed': 0, 'total': 0}, '_flushReshardingStateChange': {'failed': 0, 'total': 0}, '_flushRoutingTableCacheUpdates': {'failed': 0, 'total': 0}, '_flushRoutingTableCacheUpdatesWithWriteConcern': {'failed': 0, 'total': 0}, '_getNextSessionMods': {'failed': 0, 'total': 0}, '_getUserCacheGeneration': {'failed': 0, 'total': 0}, '_isSelf': {'failed': 0, 'total': 0}, '_killOperations': {'failed': 0, 'total': 0}, '_mergeAuthzCollections': {'failed': 0, 'total': 0}, '_migrateClone': {'failed': 0, 'total': 0}, '_recvChunkAbort': {'failed': 0, 'total': 0}, '_recvChunkCommit': {'failed': 0, 'total': 0}, '_recvChunkStart': {'failed': 0, 'total': 0}, '_recvChunkStatus': {'failed': 0, 'total': 0}, '_shardsvrAbortReshardCollection': {'failed': 0, 'total': 0}, '_shardsvrCleanupReshardCollection': {'failed': 0, 'total': 0}, '_shardsvrCloneCatalogData': {'failed': 0, 'total': 0}, '_shardsvrCommitReshardCollection': {'failed': 0, 'total': 0}, '_shardsvrCreateCollection': {'failed': 0, 'total': 0}, '_shardsvrCreateCollectionParticipant': {'failed': 0, 'total': 0}, '_shardsvrDropCollection': {'failed': 0, 'total': 0}, '_shardsvrDropCollectionParticipant': {'failed': 0, 'total': 0}, '_shardsvrDropDatabase': {'failed': 0, 'total': 0}, '_shardsvrDropDatabaseParticipant': {'failed': 0, 'total': 0}, '_shardsvrMovePrimary': {'failed': 0, 'total': 0}, '_shardsvrRefineCollectionShardKey': {'failed': 0, 'total': 0}, '_shardsvrRenameCollection': {'failed': 0, 'total': 0}, '_shardsvrRenameCollectionParticipant': {'failed': 0, 'total': 0}, '_shardsvrRenameCollectionParticipantUnblock': {'failed': 0, 'total': 0}, '_shardsvrReshardCollection': {'failed': 0, 'total': 0}, '_shardsvrReshardingOperationTime': {'failed': 0, 'total': 0}, '_shardsvrShardCollection': {'failed': 0, 'total': 0}, '_transferMods': {'failed': 0, 'total': 0}, 'abortTransaction': {'failed': 0, 'total': 0}, 'aggregate': {'failed': 0, 'total': 9}, 'appendOplogNote': {'failed': 0, 'total': 0}, 'applyOps': {'failed': 0, 'total': 0}, 'authenticate': {'failed': 0, 'total': 0}, 'availableQueryOptions': {'failed': 0, 'total': 0}, 'buildInfo': {'failed': 0, 'total': 11}, 'checkShardingIndex': {'failed': 0, 'total': 0}, 'cleanupOrphaned': {'failed': 0, 'total': 0}, 'cloneCollectionAsCapped': {'failed': 0, 'total': 0}, 'collMod': {'failed': 0, 'total': 0}, 'collStats': {'failed': 0, 'total': 4}, 'commitTransaction': {'failed': 0, 'total': 0}, 'compact': {'failed': 0, 'total': 0}, 'connPoolStats': {'failed': 0, 'total': 0}, 'connPoolSync': {'failed': 0, 'total': 0}, 'connectionStatus': {'failed': 0, 'total': 9}, 'convertToCapped': {'failed': 0, 'total': 0}, 'coordinateCommitTransaction': {'failed': 0, 'total': 0}, 'count': {'failed': 0, 'total': 0}, 'create': {'failed': 0, 'total': 1}, 'createIndexes': {'failed': 0, 'total': 0}, 'createRole': {'failed': 0, 'total': 0}, 'createUser': {'failed': 0, 'total': 0}, 'currentOp': {'failed': 0, 'total': 3}, 'dataSize': {'failed': 0, 'total': 0}, 'dbHash': {'failed': 0, 'total': 0}, 'dbStats': {'failed': 0, 'total': 40}, 'delete': {'failed': 0, 'total': 1}, 'distinct': {'failed': 0, 'total': 0}, 'donorAbortMigration': {'failed': 0, 'total': 0}, 'donorForgetMigration': {'failed': 0, 'total': 0}, 'donorStartMigration': {'failed': 0, 'total': 0}, 'driverOIDTest': {'failed': 0, 'total': 0}, 'drop': {'failed': 0, 'total': 0}, 'dropAllRolesFromDatabase': {'failed': 0, 'total': 0}, 'dropAllUsersFromDatabase': {'failed': 0, 'total': 0}, 'dropConnections': {'failed': 0, 'total': 0}, 'dropDatabase': {'failed': 0, 'total': 0}, 'dropIndexes': {'failed': 0, 'total': 0}, 'dropRole': {'failed': 0, 'total': 0}, 'dropUser': {'failed': 0, 'total': 0}, 'endSessions': {'failed': 0, 'total': 1}, 'explain': {'failed': 0, 'total': 0}, 'features': {'failed': 0, 'total': 0}, 'filemd5': {'failed': 0, 'total': 0}, 'find': {'failed': 0, 'total': 76}, 'findAndModify': {'arrayFilters': 0, 'failed': 0, 'pipeline': 0, 'total': 0}, 'flushRouterConfig': {'failed': 0, 'total': 0}, 'fsync': {'failed': 0, 'total': 0}, 'fsyncUnlock': {'failed': 0, 'total': 0}, 'getCmdLineOpts': {'failed': 0, 'total': 11}, 'getDatabaseVersion': {'failed': 0, 'total': 0}, 'getDefaultRWConcern': {'failed': 0, 'total': 0}, 'getDiagnosticData': {'failed': 0, 'total': 0}, 'getFreeMonitoringStatus': {'failed': 0, 'total': 0}, 'getLastError': {'failed': 0, 'total': 0}, 'getLog': {'failed': 0, 'total': 0}, 'getMore': {'failed': 0, 'total': 0}, 'getParameter': {'failed': 0, 'total': 11}, 'getShardMap': {'failed': 0, 'total': 0}, 'getShardVersion': {'failed': 0, 'total': 0}, 'getnonce': {'failed': 0, 'total': 0}, 'grantPrivilegesToRole': {'failed': 0, 'total': 0}, 'grantRolesToRole': {'failed': 0, 'total': 0}, 'grantRolesToUser': {'failed': 0, 'total': 0}, 'hello': {'failed': 85, 'total': 7190}, 'hostInfo': {'failed': 0, 'total': 8}, 'insert': {'failed': 0, 'total': 10}, 'internalRenameIfOptionsAndIndexesMatch': {'failed': 0, 'total': 0}, 'invalidateUserCache': {'failed': 0, 'total': 0}, 'isMaster': {'failed': 0, 'total': 6113}, 'killAllSessions': {'failed': 0, 'total': 0}, 'killAllSessionsByPattern': {'failed': 0, 'total': 0}, 'killCursors': {'failed': 0, 'total': 0}, 'killOp': {'failed': 0, 'total': 0}, 'killSessions': {'failed': 0, 'total': 0}, 'listCollections': {'failed': 0, 'total': 44}, 'listCommands': {'failed': 0, 'total': 0}, 'listDatabases': {'failed': 0, 'total': 6}, 'listIndexes': {'failed': 0, 'total': 108}, 'lockInfo': {'failed': 0, 'total': 0}, 'logRotate': {'failed': 0, 'total': 0}, 'logout': {'failed': 0, 'total': 0}, 'mapReduce': {'failed': 0, 'total': 0}, 'mergeChunks': {'failed': 0, 'total': 0}, 'moveChunk': {'failed': 0, 'total': 0}, 'ping': {'failed': 0, 'total': 0}, 'planCacheClear': {'failed': 0, 'total': 0}, 'planCacheClearFilters': {'failed': 0, 'total': 0}, 'planCacheListFilters': {'failed': 0, 'total': 0}, 'planCacheSetFilter': {'failed': 0, 'total': 0}, 'prepareTransaction': {'failed': 0, 'total': 0}, 'profile': {'failed': 0, 'total': 0}, 'reIndex': {'failed': 0, 'total': 0}, 'recipientForgetMigration': {'failed': 0, 'total': 0}, 'recipientSyncData': {'failed': 0, 'total': 0}, 'refreshSessions': {'failed': 0, 'total': 0}, 'renameCollection': {'failed': 0, 'total': 0}, 'repairDatabase': {'failed': 0, 'total': 0}, 'replSetAbortPrimaryCatchUp': {'failed': 0, 'total': 0}, 'replSetFreeze': {'failed': 0, 'total': 0}, 'replSetGetConfig': {'failed': 0, 'total': 0}, 'replSetGetRBID': {'failed': 0, 'total': 0}, 'replSetGetStatus': {'failed': 0, 'total': 0}, 'replSetHeartbeat': {'failed': 0, 'total': 0}, 'replSetInitiate': {'failed': 0, 'total': 0}, 'replSetMaintenance': {'failed': 0, 'total': 0}, 'replSetReconfig': {'failed': 0, 'total': 0}, 'replSetRequestVotes': {'failed': 0, 'total': 0}, 'replSetResizeOplog': {'failed': 0, 'total': 0}, 'replSetStepDown': {'failed': 0, 'total': 0}, 'replSetStepDownWithForce': {'failed': 0, 'total': 0}, 'replSetStepUp': {'failed': 0, 'total': 0}, 'replSetSyncFrom': {'failed': 0, 'total': 0}, 'replSetUpdatePosition': {'failed': 0, 'total': 0}, 'revokePrivilegesFromRole': {'failed': 0, 'total': 0}, 'revokeRolesFromRole': {'failed': 0, 'total': 0}, 'revokeRolesFromUser': {'failed': 0, 'total': 0}, 'rolesInfo': {'failed': 0, 'total': 0}, 'rotateCertificates': {'failed': 0, 'total': 0}, 'saslContinue': {'failed': 0, 'total': 0}, 'saslStart': {'failed': 1, 'total': 1}, 'serverStatus': {'failed': 0, 'total': 85}, 'setDefaultRWConcern': {'failed': 0, 'total': 0}, 'setFeatureCompatibilityVersion': {'failed': 0, 'total': 0}, 'setFreeMonitoring': {'failed': 0, 'total': 0}, 'setIndexCommitQuorum': {'failed': 0, 'total': 0}, 'setParameter': {'failed': 0, 'total': 0}, 'setShardVersion': {'failed': 0, 'total': 0}, 'shardingState': {'failed': 0, 'total': 0}, 'shutdown': {'failed': 0, 'total': 0}, 'splitChunk': {'failed': 0, 'total': 0}, 'splitVector': {'failed': 0, 'total': 0}, 'startRecordingTraffic': {'failed': 0, 'total': 0}, 'startSession': {'failed': 0, 'total': 0}, 'stopRecordingTraffic': {'failed': 0, 'total': 0}, 'top': {'failed': 0, 'total': 3}, 'update': {'arrayFilters': 0, 'failed': 0, 'pipeline': 131, 'total': 24}, 'updateRole': {'failed': 0, 'total': 0}, 'updateUser': {'failed': 0, 'total': 0}, 'usersInfo': {'failed': 0, 'total': 0}, 'validate': {'failed': 0, 'total': 0}, 'validateDBMetadata': {'failed': 0, 'total': 0}, 'voteCommitIndexBuild': {'failed': 0, 'total': 0}, 'waitForFailPoint': {'failed': 0, 'total': 0}, 'whatsmyuri': {'failed': 0, 'total': 0}}, 'cursor': {'moreThanOneBatch': 0, 'timedOut': 0, 'totalOpened': 12, 'lifespan': {'greaterThanOrEqual10Minutes': 0, 'lessThan10Minutes': 0, 'lessThan15Seconds': 0, 'lessThan1Minute': 0, 'lessThan1Second': 12, 'lessThan30Seconds': 0, 'lessThan5Seconds': 0}, 'open': {'noTimeout': 0, 'pinned': 0, 'total': 0}}, 'document': {'deleted': 0, 'inserted': 10, 'returned': 213, 'updated': 0}, 'dotsAndDollarsFields': {'inserts': 0, 'updates': 0}, 'getLastError': {'wtime': {'num': 0, 'totalMillis': 0}, 'wtimeouts': 0, 'default': {'unsatisfiable': 0, 'wtimeouts': 0}}, 'mongos': {'cursor': {'moreThanOneBatch': 0, 'totalOpened': 0}}, 'operation': {'scanAndOrder': 0, 'writeConflicts': 0}, 'operatorCounters': {'expressions': {'$_internalJsEmit': 0, '$abs': 0, '$acos': 0, '$acosh': 0, '$add': 0, '$allElementsTrue': 0, '$and': 0, '$anyElementTrue': 0, '$arrayElemAt': 0, '$arrayToObject': 0, '$asin': 0, '$asinh': 0, '$atan': 0, '$atan2': 0, '$atanh': 0, '$avg': 0, '$binarySize': 0, '$bsonSize': 0, '$ceil': 0, '$cmp': 0, '$concat': 0, '$concatArrays': 0, '$cond': 0, '$const': 0, '$convert': 0, '$cos': 0, '$cosh': 0, '$dateAdd': 0, '$dateDiff': 0, '$dateFromParts': 0, '$dateFromString': 0, '$dateSubtract': 0, '$dateToParts': 0, '$dateToString': 0, '$dateTrunc': 0, '$dayOfMonth': 0, '$dayOfWeek': 0, '$dayOfYear': 0, '$degreesToRadians': 0, '$divide': 0, '$eq': 0, '$exp': 0, '$filter': 0, '$first': 0, '$floor': 0, '$function': 0, '$getField': 0, '$gt': 0, '$gte': 0, '$hour': 0, '$ifNull': 0, '$in': 0, '$indexOfArray': 0, '$indexOfBytes': 0, '$indexOfCP': 0, '$isArray': 0, '$isNumber': 0, '$isoDayOfWeek': 0, '$isoWeek': 0, '$isoWeekYear': 0, '$last': 0, '$let': 0, '$literal': 0, '$ln': 0, '$log': 0, '$log10': 0, '$lt': 0, '$lte': 0, '$ltrim': 0, '$map': 0, '$max': 0, '$mergeObjects': 0, '$meta': 0, '$millisecond': 0, '$min': 0, '$minute': 0, '$mod': 0, '$month': 0, '$multiply': 0, '$ne': 0, '$not': 0, '$objectToArray': 0, '$or': 0, '$pow': 0, '$radiansToDegrees': 0, '$rand': 0, '$range': 0, '$reduce': 0, '$regexFind': 0, '$regexFindAll': 0, '$regexMatch': 0, '$replaceAll': 0, '$replaceOne': 0, '$reverseArray': 0, '$round': 0, '$rtrim': 0, '$second': 0, '$setDifference': 0, '$setEquals': 0, '$setField': 0, '$setIntersection': 0, '$setIsSubset': 0, '$setUnion': 0, '$sin': 0, '$sinh': 0, '$size': 0, '$slice': 0, '$split': 0, '$sqrt': 0, '$stdDevPop': 0, '$stdDevSamp': 0, '$strLenBytes': 0, '$strLenCP': 0, '$strcasecmp': 0, '$substr': 0, '$substrBytes': 0, '$substrCP': 0, '$subtract': 0, '$sum': 0, '$switch': 0, '$tan': 0, '$tanh': 0, '$toBool': 0, '$toDate': 0, '$toDecimal': 0, '$toDouble': 0, '$toHashedIndexKey': 0, '$toInt': 0, '$toLong': 0, '$toLower': 0, '$toObjectId': 0, '$toString': 0, '$toUpper': 0, '$trim': 0, '$trunc': 0, '$type': 0, '$unsetField': 0, '$week': 0, '$year': 0, '$zip': 0}}, 'query': {'planCacheTotalSizeEstimateBytes': 0, 'updateOneOpStyleBroadcastWithExactIDCount': 0}, 'queryExecutor': {'scanned': 22, 'scannedObjects': 204, 'collectionScans': {'nonTailable': 24, 'total': 24}}, 'record': {'moves': 0}, 'repl': {'executor': {'pool': {'inProgressCount': 0}, 'queues': {'networkInProgress': 0, 'sleepers': 0}, 'unsignaledEvents': 0, 'shuttingDown': False, 'networkInterface': 'DEPRECATED: getDiagnosticString is deprecated in NetworkInterfaceTL'}, 'apply': {'attemptsToBecomeSecondary': 0, 'batchSize': 0, 'batches': {'num': 0, 'totalMillis': 0}, 'ops': 0}, 'buffer': {'count': 0, 'maxSizeBytes': 0, 'sizeBytes': 0}, 'initialSync': {'completed': 0, 'failedAttempts': 0, 'failures': 0}, 'network': {'bytes': 0, 'getmores': {'num': 0, 'totalMillis': 0, 'numEmptyBatches': 0}, 'notPrimaryLegacyUnacknowledgedWrites': 0, 'notPrimaryUnacknowledgedWrites': 0, 'oplogGetMoresProcessed': {'num': 0, 'totalMillis': 0}, 'ops': 0, 'readersCreated': 0, 'replSetUpdatePosition': {'num': 0}}, 'reconfig': {'numAutoReconfigsForRemovalOfNewlyAddedFields': 0}, 'stateTransition': {'lastStateTransition': '', 'userOperationsKilled': 0, 'userOperationsRunning': 0}, 'syncSource': {'numSelections': 0, 'numSyncSourceChangesDueToSignificantlyCloserNode': 0, 'numTimesChoseDifferent': 0, 'numTimesChoseSame': 0, 'numTimesCouldNotFind': 0}}, 'ttl': {'deletedDocuments': 114, 'passes': 260}}, 'ok': 1.0}\n"
     ]
    }
   ],
   "source": [
    "client = Mongoconnect()\n",
    "mydb = client[\"Tweet_database\"]\n",
    "mycol = mydb[\"raw_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bba7815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1533145807440031745': ['jdcocchiarella',\n",
       "  'lol',\n",
       "  'as',\n",
       "  'if',\n",
       "  'every',\n",
       "  'chick-fil-a',\n",
       "  'location',\n",
       "  'doesn',\n",
       "  '’',\n",
       "  't',\n",
       "  'employ',\n",
       "  'at',\n",
       "  'least',\n",
       "  '3',\n",
       "  'gay',\n",
       "  'per',\n",
       "  'shift',\n",
       "  'you',\n",
       "  'love',\n",
       "  'talk',\n",
       "  'about',\n",
       "  'their',\n",
       "  'customer',\n",
       "  'but',\n",
       "  'completely',\n",
       "  'ignore',\n",
       "  'all',\n",
       "  'the',\n",
       "  'gay',\n",
       "  'ppl',\n",
       "  'who',\n",
       "  'work',\n",
       "  'there'],\n",
       " '1533145807007907846': ['rt',\n",
       "  'jdcocchiarella',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'up',\n",
       "  'chick-fil-a',\n",
       "  'for',\n",
       "  'pride',\n",
       "  'month',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life'],\n",
       " '1533145793435156481': ['not',\n",
       "  'me',\n",
       "  'finally',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'try',\n",
       "  'chick-fil-a'],\n",
       " '1533145786032304130': ['she',\n",
       "  'good',\n",
       "  'not',\n",
       "  'ever',\n",
       "  'question',\n",
       "  'shit',\n",
       "  'i',\n",
       "  'just',\n",
       "  'give',\n",
       "  'her',\n",
       "  'the',\n",
       "  'last',\n",
       "  'bite',\n",
       "  'of',\n",
       "  'my',\n",
       "  'chick',\n",
       "  'fil',\n",
       "  'a',\n",
       "  'sandwich'],\n",
       " '1533145765018836992': ['rt',\n",
       "  'studiodeliciou1',\n",
       "  'this',\n",
       "  'chick',\n",
       "  'fil',\n",
       "  'a',\n",
       "  'copycat',\n",
       "  'be',\n",
       "  'just',\n",
       "  'as',\n",
       "  'good',\n",
       "  'good',\n",
       "  'than',\n",
       "  'the',\n",
       "  'original',\n",
       "  'and',\n",
       "  'super',\n",
       "  'easy',\n",
       "  'to',\n",
       "  'make',\n",
       "  'http',\n",
       "  '//t.co/rkqksoxqnw',\n",
       "  'h…'],\n",
       " '1533145723247546368': ['rt',\n",
       "  'jdcocchiarella',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'up',\n",
       "  'chick-fil-a',\n",
       "  'for',\n",
       "  'pride',\n",
       "  'month',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life'],\n",
       " '1533145718826803201': ['rt',\n",
       "  'jdcocchiarella',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'up',\n",
       "  'chick-fil-a',\n",
       "  'for',\n",
       "  'pride',\n",
       "  'month',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life'],\n",
       " '1533145665806688257': ['rt',\n",
       "  'jdcocchiarella',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'up',\n",
       "  'chick-fil-a',\n",
       "  'for',\n",
       "  'pride',\n",
       "  'month',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life'],\n",
       " '1533145652645056515': ['rt',\n",
       "  'jdcocchiarella',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'up',\n",
       "  'chick-fil-a',\n",
       "  'for',\n",
       "  'pride',\n",
       "  'month',\n",
       "  'you',\n",
       "  'can',\n",
       "  'give',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life'],\n",
       " '1533145630754738177': ['i',\n",
       "  'want',\n",
       "  'chick-fil-a',\n",
       "  'but',\n",
       "  'the',\n",
       "  'way',\n",
       "  'gas',\n",
       "  'look',\n",
       "  'ima',\n",
       "  'just',\n",
       "  'get',\n",
       "  'wendy',\n",
       "  \"'s\"]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "data = dict()\n",
    "for i in mycol.find():\n",
    "    custom_tokens = remove_noise(word_tokenize(i[\"text\"]))\n",
    "    data[i[\"id\"]] = custom_tokens\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9c9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "Accuracy is: 0.9946666666666667\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =    990.0 : 1.0\n",
      "                follower = True           Positi : Negati =     23.5 : 1.0\n",
      "                     bam = True           Positi : Negati =     23.3 : 1.0\n",
      "                     sad = True           Negati : Positi =     22.2 : 1.0\n",
      "                    glad = True           Positi : Negati =     19.4 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.9 : 1.0\n",
      "                  arrive = True           Positi : Negati =     14.8 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.5 : 1.0\n",
      "                followed = True           Negati : Positi =     14.4 : 1.0\n",
      "                    blog = True           Positi : Negati =     13.5 : 1.0\n",
      "None\n",
      "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))\n",
    "\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80a2115a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1533145807440031745': 'Positive',\n",
       " '1533145807007907846': 'Positive',\n",
       " '1533145793435156481': 'Negative',\n",
       " '1533145786032304130': 'Negative',\n",
       " '1533145765018836992': 'Positive',\n",
       " '1533145723247546368': 'Positive',\n",
       " '1533145718826803201': 'Positive',\n",
       " '1533145665806688257': 'Positive',\n",
       " '1533145652645056515': 'Positive',\n",
       " '1533145630754738177': 'Negative'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = dict()\n",
    "for k,item in data.items():\n",
    "    result[k] = classifier.classify(dict([token, True] for token in item))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e850960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kevalharkhani/PycharmProjects/Data Engineering/Project/dags/kafka'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "952f2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pkl', 'wb') as files:\n",
    "    pickle.dump(classifier, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afdfa7b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not '_io.TextIOWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4c/61584nq90bgbl8cc78qxhknw0000gn/T/ipykernel_20239/2125984972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"model_pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not '_io.TextIOWrapper'"
     ]
    }
   ],
   "source": [
    "classifier = pickle.loads(open(f\"model_pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79e05681",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pkl' , 'rb') as f:\n",
    "    lr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56523466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/kevalharkhani/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
